# -*- coding: utf-8 -*-
"""Copy_of_Copie_de_P2M.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T--M86vuQoqPbZX9_T67Hgtj9cOT0TiQ

# Importing Libraries
"""

import math
import numpy as np
import pandas as pd 
from sklearn.metrics import jaccard_similarity_score
import cv2
import tensorflow as tf
import os
import re
import random
import shutil
import matplotlib.pyplot as plt

from sklearn.cluster import MeanShift, estimate_bandwidth
from zipfile import ZipFile
from tqdm import tqdm
from itertools import chain
from skimage.io import imread, imshow, imread_collection, concatenate_images
from skimage.transform import resize
from skimage.morphology import label

from keras.models import Model, load_model
from keras.layers import Input
from keras.optimizers import Adam
from keras.layers.merge import concatenate
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout
from keras.layers.core import Lambda
from keras.callbacks import EarlyStopping, ModelCheckpoint

import sys
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='skimage')

"""# Importing Data"""

from google.colab import drive
drive.mount('/content/drive')

path_train = '/content/drive/MyDrive/p2m/stage1_train.zip'
with ZipFile(path_train) as zf:
    zf.extractall()

path_Y_train = '/content/drive/MyDrive/p2m/stage1_train_labels.csv.zip'
with ZipFile(path_Y_train) as zf:
    zf.extractall()

trainlb=pd.read_csv('stage1_train_labels.csv')

trainlb.head()

train_ids = trainlb.ImageId.unique()

train_ids.shape

"""# Data Processing

## Splitting the Data into Train and Test
"""

test_ids = train_ids[int(train_ids.shape[0]*0.7):]
train_ids = train_ids[:int(train_ids.shape[0]*0.7)]

train_ids.shape

test_ids.shape

"""## Data Processing"""

IMG_WIDTH = 128
IMG_HEIGHT = 128
IMG_CHANNELS = 3
TRAIN_PATH="/content/"

X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)
Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)

X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)
Y_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)

print('Resizing training images and masks')
for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):   
    path = TRAIN_PATH + id_
    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]  
    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)
    X_train[n] = img  #Fill empty X_train with values from img
    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)
    for mask_file in next(os.walk(path + '/masks/'))[2]:
        mask_ = imread(path + '/masks/' + mask_file)
        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  
                                      preserve_range=True), axis=-1)
        mask = np.maximum(mask, mask_)  
            
    Y_train[n] = mask

print('Resizing training images and masks')
for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):   
    path = TRAIN_PATH + id_
    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]  
    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)
    X_test[n] = img  #Fill empty X_test with values from img
    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)
    for mask_file in next(os.walk(path + '/masks/'))[2]:
        mask_ = imread(path + '/masks/' + mask_file)
        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True), axis=-1)
        mask = np.maximum(mask, mask_)  
            
    Y_test[n] = mask

"""## Data Visualization

Exemple d'image et son masque :
"""

image_x=random.randint(0,len(train_ids))
imshow(X_train[image_x])
plt.show()
imshow(np.squeeze(Y_train[image_x]))

plt.show()

"""# Testing Models

## Classic Methods

### Otsu's method

#### Defining Otsu Model
"""

otsu_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH ), dtype=np.uint8)

for n, id in enumerate(train_ids):
  im_gray = cv2.cvtColor(X_train[n], cv2.COLOR_BGR2GRAY)
  th, im_gray_th_otsu = cv2.threshold(im_gray, 0, 255, cv2.THRESH_OTSU)
  otsu_train[n]=im_gray_th_otsu

"""#### Evaluation on Dataset"""

otsu_jaccard = np.zeros(len(train_ids))

otsu_dice = np.zeros(len(train_ids))

for n, id in enumerate(train_ids):

  # Compute Jaccard index
  img_true=np.array(Y_train[n]).ravel()
  img_pred=np.array(otsu_train[n]).ravel()
  otsu_jaccard[n] = jaccard_similarity_score(img_true, img_pred)

   # Compute Dice coefficient
  true_mask = np.asarray(img_true).astype(np.bool)
  pred_mask = np.asarray(img_pred).astype(np.bool)
  im_sum = true_mask.sum() + pred_mask.sum()
  if im_sum == 0:
      dsc = 1
  intersection = np.logical_and(true_mask, pred_mask)
  otsu_dice[n] = 2. * intersection.sum() / im_sum

print(otsu_jaccard.mean())
print(otsu_dice.mean())

u, indices = np.unique(otsu_train, return_inverse=True)
otsu_train_norm = [i / 255 for i in otsu_train]

import seaborn as sns

FP = len(np.where(otsu_train_norm - np.squeeze(Y_train)  == -1)[0])
FN = len(np.where(otsu_train_norm - np.squeeze(Y_train)  == 1)[0])
TP = len(np.where(otsu_train_norm + np.squeeze(Y_train) ==2)[0])
TN = len(np.where(otsu_train_norm + np.squeeze(Y_train) == 0)[0])
cmat = [[TP, FN], [FP, TN]]

plt.figure(figsize = (6,6))
sns.heatmap(cmat/np.sum(cmat), cmap="Reds", annot=True,fmt = '.2%', square=1, linewidth=2.)
plt.xlabel("otsu predicted values")
plt.ylabel("real values")
plt.title("Confusion Matrix on Otsu Train Set")
plt.show()

"""#### Testing set"""

otsu_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH ), dtype=np.uint8)

for n, id in enumerate(test_ids):
  im_gray = cv2.cvtColor(X_test[n], cv2.COLOR_BGR2GRAY)
  th, im_gray_th_otsu = cv2.threshold(im_gray, 0, 255, cv2.THRESH_OTSU)
  otsu_test[n]=im_gray_th_otsu

test_otsu_jaccard= np.zeros(len(test_ids))
test_otsu_dice = np.zeros(len(test_ids))
for n, id in enumerate(test_ids):

  # Compute Jaccard index
  img_true=np.array(Y_test[n]).ravel()
  img_pred=np.array(otsu_test[n]).ravel()
  test_otsu_jaccard[n] = jaccard_similarity_score(img_true, img_pred)

   # Compute Dice coefficient
  true_mask = np.asarray(img_true).astype(np.bool)
  pred_mask = np.asarray(img_pred).astype(np.bool)
  im_sum = true_mask.sum() + pred_mask.sum()
  if im_sum == 0:
      dsc = 1
  intersection = np.logical_and(true_mask, pred_mask)
  test_otsu_dice[n] = 2. * intersection.sum() / im_sum

print(test_otsu_jaccard.mean())
print(test_otsu_dice.mean())

otsu_test_norm = [i / 255 for i in otsu_test]

import seaborn as sns

FP = len(np.where(otsu_test_norm - np.squeeze(Y_test)  == -1)[0])
FN = len(np.where(otsu_test_norm - np.squeeze(Y_test)  == 1)[0])
TP = len(np.where(otsu_test_norm + np.squeeze(Y_test) ==2)[0])
TN = len(np.where(otsu_test_norm + np.squeeze(Y_test) == 0)[0])
cmat = [[TP, FN], [FP, TN]]

plt.figure(figsize = (6,6))
sns.heatmap(cmat/np.sum(cmat), cmap="Reds", annot=True,fmt = '.2%', square=1, linewidth=2.)
plt.xlabel("otsu predicted values")
plt.ylabel("real values")
plt.title("Confusion Matrix on Otsu Test Set")
plt.show()

"""#### Evaluation on one Image"""

image_x=random.randint(0,len(train_ids))
image_x = 5
imshow(X_train[image_x])
plt.show()
imshow(otsu_train[image_x])
plt.show()
imshow(np.squeeze(Y_train[image_x]))
plt.show()

"""Calculation of the Jaccard Index and Dice for one Data sample"""

img_true=np.array(Y_train[5]).ravel()
img_pred=np.array(otsu_train[5]).ravel()
print('Jaccard Score pour une image avec Otsu: ', otsu_jaccard[5])
print('Dice Score pour une image avec Otsu: ', otsu_dice[5])

"""###  MeanShift

#### Defining MeansShift Model
"""

meanshift_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH ), dtype=np.uint8)

for n, id in enumerate(train_ids):
  flat_image=np.reshape(X_train[n], [-1, 3])
  bandwidth2 = estimate_bandwidth(flat_image, quantile=.005, n_samples=1000)
  ms = MeanShift(bandwidth2, bin_seeding=True)
  ms.fit(flat_image)
  #grayImage = cv2.cvtColor(np.reshape(ms.labels_, [128,128]), cv2.COLOR_BGR2GRAY)
  #(thresh, blackAndWhiteImage) = cv2.threshold(grayImage, 10, 255, cv2.THRESH_BINARY)
  meanshift_train[n] = np.reshape(ms.labels_, [128,128])
  (thresh, blackAndWhiteImage) = cv2.threshold(meanshift_train[n], 10, 255, cv2.THRESH_BINARY)
  meanshift_train[n]=blackAndWhiteImage

"""#### Evaluation on Dataset"""

meanshift_jaccard = np.zeros(len(train_ids))
meanshift_dice = np.zeros(len(train_ids))

for n, id in enumerate(train_ids):

  # Compute Jaccard index
  img_true=np.array(Y_train[n]).ravel()
  img_pred=np.array(meanshift_train[n]).ravel()
  meanshift_jaccard[n] = jaccard_similarity_score(img_true, img_pred)

   # Compute Dice coefficient
  true_mask = np.asarray(img_true).astype(np.bool)
  pred_mask = np.asarray(img_pred).astype(np.bool)
  im_sum = true_mask.sum() + pred_mask.sum()
  if im_sum == 0:
      dsc = 1
  intersection = np.logical_and(true_mask, pred_mask)
  meanshift_dice[n] = 2. * intersection.sum() / im_sum

print(meanshift_jaccard.mean())
print(meanshift_dice.mean())

meanshift_train_norm = [i / 255 for i in meanshift_train]

import seaborn as sns

FP = len(np.where(meanshift_train_norm - np.squeeze(Y_train)  == -1)[0])
FN = len(np.where(meanshift_train_norm - np.squeeze(Y_train)  == 1)[0])
TP = len(np.where(meanshift_train_norm + np.squeeze(Y_train) ==2)[0])
TN = len(np.where(meanshift_train_norm + np.squeeze(Y_train) == 0)[0])
cmat = [[TP, FN], [FP, TN]]

plt.figure(figsize = (6,6))
sns.heatmap(cmat/np.sum(cmat), cmap="Reds", annot=True,fmt = '.2%', square=1, linewidth=2.)
plt.xlabel("otsu predicted values")
plt.ylabel("real values")
plt.title("Confusion Matrix on Otsu Train Set")
plt.show()

"""### Testing set"""

meanshift_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH ), dtype=np.uint8)

for n, id in enumerate(test_ids):
  flat_image=np.reshape(X_test[n], [-1, 3])
  bandwidth2 = estimate_bandwidth(flat_image, quantile=.005, n_samples=1000)
  ms = MeanShift(bandwidth2, bin_seeding=True)
  ms.fit(flat_image)
  #grayImage = cv2.cvtColor(np.reshape(ms.labels_, [128,128]), cv2.COLOR_BGR2GRAY)
  #(thresh, blackAndWhiteImage) = cv2.threshold(grayImage, 10, 255, cv2.THRESH_BINARY)
  meanshift_test[n] = np.reshape(ms.labels_, [128,128])
  (thresh, blackAndWhiteImage) = cv2.threshold(meanshift_test[n], 10, 255, cv2.THRESH_BINARY)
  meanshift_test[n]=blackAndWhiteImage

test_meanshift_jaccard = np.zeros(len(test_ids))
test_meanshift_dice = np.zeros(len(test_ids))

for n, id in enumerate(test_ids):

  # Compute Jaccard index
  img_true=np.array(Y_test[n]).ravel()
  img_pred=np.array(meanshift_test[n]).ravel()
  test_meanshift_jaccard[n] = jaccard_similarity_score(img_true, img_pred)

   # Compute Dice coefficient
  true_mask = np.asarray(img_true).astype(np.bool)
  pred_mask = np.asarray(img_pred).astype(np.bool)
  im_sum = true_mask.sum() + pred_mask.sum()
  if im_sum == 0:
      dsc = 1
  intersection = np.logical_and(true_mask, pred_mask)
  test_meanshift_dice[n] = 2. * intersection.sum() / im_sum

print(test_meanshift_jaccard.mean())
print(test_meanshift_dice.mean())

meanshift_test_norm = [i / 255 for i in meanshift_test]

import seaborn as sns

FP = len(np.where(meanshift_test_norm - np.squeeze(Y_test)  == -1)[0])
FN = len(np.where(meanshift_test_norm - np.squeeze(Y_test)  == 1)[0])
TP = len(np.where(meanshift_test_norm + np.squeeze(Y_test) ==2)[0])
TN = len(np.where(meanshift_test_norm + np.squeeze(Y_test) == 0)[0])
cmat = [[TP, FN], [FP, TN]]

plt.figure(figsize = (6,6))
sns.heatmap(cmat/np.sum(cmat), cmap="Reds", annot=True,fmt = '.2%', square=1, linewidth=2.)
plt.xlabel("otsu predicted values")
plt.ylabel("real values")
plt.title("Confusion Matrix on Meanshift Test Set")
plt.show()

"""#### Evaluation on one Image"""

image_x=random.randint(0,len(train_ids))
image_x = 5
imshow(X_train[image_x])
plt.show()
imshow(meanshift_train[image_x])
plt.show()
imshow(np.squeeze(Y_train[image_x]))
plt.show()

img_true=np.array(Y_train[5]).ravel()
img_pred=np.array(meanshift_train[5]).ravel()
print('Jaccard Score pour une image avec Meanshift: ', meanshift_jaccard[5])
print('Dice Score pour une image avec Meanshift: ', meanshift_dice[5])

"""## Deep Learning Model

### Model Definition

Defining Dice and Jaccard metrics for Deep Learning Model
"""

print("definition of the dice metric")
from keras import backend as K
smooth = 1
def dice_coef(y_true, y_pred):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

print("definition of the jaccard metric")
from keras import backend as K
from keras import losses
import numpy as np
import tensorflow as tf
def JI(y_true, y_pred):
    y_true = K.flatten(y_true)
    y_pred = K.flatten(y_pred)

    threshold_value = 0.3

    y_pred = K.cast(K.greater(y_pred, threshold_value), K.floatx())
    fenzi = K.sum(y_true * y_pred, keepdims=True)
    # true_positives_sum = K.sum(true_positives, keepdims=True)
    fenmu = K.sum(K.cast((K.greater(y_true + y_pred, 0.8)), K.floatx()), keepdims=True)

   
    return K.mean(fenzi / fenmu, axis=-1)

#Build the model

#Input model
inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))
s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)

#Contraction path
c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)
c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)

c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)
c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)
p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)
 
c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)
c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)
p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)
 
c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)
c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)
p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)
 
c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)
c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)

#Expansive path 
u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)
u6 = tf.keras.layers.concatenate([u6, c4])
c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)
 
u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)
u7 = tf.keras.layers.concatenate([u7, c3])
c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)
c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)
 
u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)
u8 = tf.keras.layers.concatenate([u8, c2])
c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)
c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)
 
u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)
u9 = tf.keras.layers.concatenate([u9, c1], axis=3)
c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)
c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)

#Output model
outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)
model = tf.keras.Model(inputs=[inputs], outputs=[outputs])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',dice_coef,JI])
model.summary()

"""Modelization plot"""

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model_d_plot.png')

"""### Model Fitting

Calling The Tensorboard for model evaluation of parameters when fitting the model
"""

# Commented out IPython magic to ensure Python compatibility.
import datetime
# Load the TensorBoard notebook extension
# %load_ext tensorboard
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

from keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint
MCP = ModelCheckpoint('Best_points.h5',verbose=1,save_best_only=True,monitor='val_accuracy',mode='max')
ES = EarlyStopping(monitor='val_loss',min_delta=0,verbose=1,restore_best_weights = True,patience=3,mode='max')
RLP = ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.2,min_lr=0.0001)



callbacks=[tf.keras.callbacks.EarlyStopping(patience=3,monitor='val_loss'),tf.keras.callbacks.TensorBoard(log_dir='logs')]
callbacks = [callbacks]
callbacks = [tensorboard_callback]

results=model.fit(X_train,Y_train,validation_split=0.2,batch_size=32,epochs=50, callbacks=callbacks)

"""#### U-Net Model Evaluation on Train, Validation and Study of Parameters"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit

plt.plot(results.history['loss']) 
plt.plot(results.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epoch') 
plt.legend(['Train', 'Validation'], loc='upper left') 
print("Loss function :")
plt.show()

plt.plot(results.history['accuracy']) 
plt.plot(results.history['val_accuracy']) 
plt.title('Model accuracy') 
plt.ylabel('accuracy') 
plt.xlabel('Epoch') 
plt.legend(['Train', 'Validation'], loc='upper left') 
print("accuracy function :")
plt.show()

plt.plot(results.history['JI']) 
plt.plot(results.history['val_JI']) 
plt.title('Jaccard Index') 
plt.ylabel('JI') 
plt.xlabel('Epoch') 
plt.legend(['Train', 'Validation'], loc='upper left') 
print("Jaccard index :")
plt.show()

plt.plot(results.history['dice_coef']) 
plt.plot(results.history['val_dice_coef']) 
plt.title('Dice Coefficient') 
plt.ylabel('Dice') 
plt.xlabel('Epoch') 
plt.legend(['Train', 'Validation'], loc='upper left') 
print("Dice coefficient :")
plt.show()

"""### Model Prediction on Test"""

# Applying the predictions on the Test Dataset
prediction=model.predict(X_test,verbose=1)
prediction=(prediction>0.5).astype(np.uint8)

"""#### U-Net Model Evaluation on Test Dataset"""

deep_jaccard = np.zeros(len(test_ids))
deep_dice = np.zeros(len(test_ids))

for n, id in enumerate(test_ids): 

  # Compute Jaccard index
  img_true=np.array(Y_test[n]).ravel()
  img_pred=np.array(prediction[n]).ravel()
  deep_jaccard[n] = jaccard_similarity_score(img_true, img_pred)

   # Compute Dice coefficient
  true_mask = np.asarray(img_true).astype(np.bool)
  pred_mask = np.asarray(img_pred).astype(np.bool)
  im_sum = true_mask.sum() + pred_mask.sum()
  if im_sum == 0:
      dsc = 1
  intersection = np.logical_and(true_mask, pred_mask)
  deep_dice[n] = 2. * intersection.sum() / im_sum

print(deep_jaccard.mean())
print(deep_dice.mean())

print('min jaccard',deep_jaccard.min())
print('min dice',deep_dice.min())
print('max jaccard',deep_jaccard.max())
print('max dice',deep_dice.max())
print('ecart type jaccard',deep_jaccard.std())
print('ecart type dice',deep_dice.std())

import seaborn as sns

FP = len(np.where(prediction - Y_test  == -1)[0])
FN = len(np.where(prediction - Y_test  == 1)[0])
TP = len(np.where(prediction + Y_test ==2)[0])
TN = len(np.where(prediction + Y_test == 0)[0])
cmat = [[TP, FN], [FP, TN]]

plt.figure(figsize = (6,6))
sns.heatmap(cmat/np.sum(cmat), cmap="Reds", annot=True,fmt = '.2%', square=1, linewidth=2.)
plt.xlabel("predictions")
plt.ylabel("real values")
plt.title("Confusion Matrix on Test Set")
plt.show()

"""### U-Net Model Evaluation on one image sample"""

image_x=random.randint(0,len(test_ids))
image_x = 5
imshow(X_test[image_x])
plt.show()
imshow(np.squeeze(prediction[image_x]))
plt.show()
imshow(np.squeeze(Y_test[image_x]))
plt.show()

"""# Comparision of Models"""

index=random.randint(0,len(test_ids))
print(index)

imshow(X_test[index])
plt.show()
imshow(otsu_test[index])
plt.show()
imshow(meanshift_test[index])
plt.show()
imshow(np.squeeze(prediction[index]))
plt.show()
imshow(np.squeeze(Y_test[index]))
plt.show()

f, axarr = plt.subplots(2,3)
axarr[0,0].imshow(X_test[index])
axarr[0,1].imshow(np.squeeze(Y_test[index]))
axarr[0,2].imshow(otsu_test[index])
axarr[1,0].imshow(meanshift_test[index])
axarr[1,1].imshow(np.squeeze(prediction[index]))

print('jaccard score pour otsu', test_otsu_jaccard[index])
print('jaccard score pour meanshift', test_meanshift_jaccard[index])
print('jaccard score pour U-Net', deep_jaccard[index])
print('Dice score pour otsu', test_otsu_dice[index])
print('Dice score pour meanshift', test_meanshift_dice[index])
print('Dice score pour U-Net', deep_dice[index])

